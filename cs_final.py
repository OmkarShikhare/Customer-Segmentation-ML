# -*- coding: utf-8 -*-
"""CS Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZJw2pajPnxwgwIQCxnx4BKsjFVb8xaj6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("clean_clusters.csv")
df.head()

"""#EDA (Exploratory Data Analysis)"""

df.shape
df.info()

df.isnull().sum()

df.duplicated().sum()

"""#Feature Engineering"""

df["Total_Spending"] = (df["MntWines"] + df["MntFruits"] + df["MntMeatProducts"] + df["MntGoldProds"])

"""#Visuals"""

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.show()

df.hist(figsize=(15,10))
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(data=df[[
    "Income",
    "Recency",
    "MntWines",
    "MntMeatProducts",
    "NumWebPurchases",
    "NumStorePurchases"]])
plt.xticks(rotation=45)
plt.title("Boxplot Analysis of Key Features")
plt.show()

plt.figure(figsize=(6,4))
sns.scatterplot(
    x=df["Income"],
    y=df["Total_Spending"])
plt.title("Income vs Total Spending")
plt.show()

"""#Feature Selection"""

X = df[["Income", "Recency", "NumWebPurchases","NumStorePurchases","Total_Spending"]]

"""#Feature Scaling"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

from sklearn.cluster import KMeans , DBSCAN , AgglomerativeClustering
from sklearn.metrics import silhouette_score , davies_bouldin_score

inertia = []
for k in range(2,11):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X_scaled)
    inertia.append(km.inertia_)

plt.figure(figsize=(8,5))
plt.plot(range(2,11), inertia, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("inertia")
plt.title("Elbow Method on PCA Data")
plt.show()

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

dbscan = DBSCAN(eps=0.8, min_samples=8)
dbscan_labels = dbscan.fit_predict(X_scaled)

hier = AgglomerativeClustering(n_clusters=3)
hier_labels = hier.fit_predict(X_scaled)

"""##Hyperparameter Tuning"""

scores = {}

for k in range(2,8):
  labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)
  scores[k] = silhouette_score(X_scaled, labels)

scores

eps_values = [0.3,0.5,0.7,0.9]
for eps in eps_values:
    labels = DBSCAN(eps=eps, min_samples=5).fit_predict(X_scaled)
    if len(set(labels)) > 1:
        print(eps, silhouette_score(X_scaled, labels))

"""#Evaluation Metrix"""

def evaluate(name, X, labels):
    print(f"\n{name}")
    print("Silhouette:", silhouette_score(X, labels))          #Higher is better
    print("Davies-Bouldin:", davies_bouldin_score(X, labels))  #Lower is better

evaluate("KMeans", X_scaled, kmeans_labels)
evaluate("Hierarchical", X_scaled, hier_labels)

if len(set(dbscan_labels)) > 1:
    evaluate("DBSCAN", X_scaled, dbscan_labels)

df["Cluster"] = kmeans_labels

cluster_labels = {
    0: "Regular Customers",
    1: "Price-Sensitive Customers",
    2: "High-Value Customers"
}

df["Customer_Segment"] = df["Cluster"].map(cluster_labels)

plt.figure(figsize=(8,5))
sns.scatterplot(
    x=X_pca[:,0],
    y=X_pca[:,1],
    hue=df["Customer_Segment"],
    palette="Set1"
)
plt.title("Customer Segmentation using PCA + KMeans")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

import pickle

pickle.dump(kmeans, open("kmeans_cluster_model.pkl","wb"))
pickle.dump(scaler, open("scaler.pkl","wb"))

